{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import RobertaForMaskedLM, RobertaTokenizer\n",
    "import torch\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12/01/2019 19:52:33 - INFO - transformers.configuration_utils -   loading configuration file ./output/config.json\n",
      "12/01/2019 19:52:33 - INFO - transformers.configuration_utils -   Model config {\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"finetuning_task\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"is_decoder\": false,\n",
      "  \"layer_norm_eps\": 1e-05,\n",
      "  \"max_position_embeddings\": 514,\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"num_labels\": 2,\n",
      "  \"output_attentions\": false,\n",
      "  \"output_hidden_states\": false,\n",
      "  \"output_past\": true,\n",
      "  \"pruned_heads\": {},\n",
      "  \"torchscript\": false,\n",
      "  \"type_vocab_size\": 1,\n",
      "  \"use_bfloat16\": false,\n",
      "  \"vocab_size\": 50265\n",
      "}\n",
      "\n",
      "12/01/2019 19:52:33 - INFO - transformers.modeling_utils -   loading weights file ./output/pytorch_model.bin\n",
      "12/01/2019 19:52:37 - INFO - transformers.tokenization_utils -   Model name './output/' not found in model shortcut name list (roberta-base, roberta-large, roberta-large-mnli). Assuming './output/' is a path or url to a directory containing tokenizer files.\n",
      "12/01/2019 19:52:37 - INFO - transformers.tokenization_utils -   loading file ./output/vocab.json\n",
      "12/01/2019 19:52:37 - INFO - transformers.tokenization_utils -   loading file ./output/merges.txt\n",
      "12/01/2019 19:52:37 - INFO - transformers.tokenization_utils -   loading file ./output/added_tokens.json\n",
      "12/01/2019 19:52:37 - INFO - transformers.tokenization_utils -   loading file ./output/special_tokens_map.json\n",
      "12/01/2019 19:52:37 - INFO - transformers.tokenization_utils -   loading file ./output/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "model = RobertaForMaskedLM.from_pretrained('./output/')\n",
    "tokenizer = RobertaTokenizer.from_pretrained('./output/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from random import choice\n",
    "\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_intents = os.path.join('data', 'raw')\n",
    "intents = os.listdir(path_to_intents)\n",
    "get_path = lambda x: os.path.join('data', 'raw', x, x + \".csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('helpers')\n",
    "from analyze import questions, entities, get_data\n",
    "data = get_data()\n",
    "os.chdir('..')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "text = defaultdict(list)\n",
    "encoded_text = defaultdict(list)\n",
    "\n",
    "ans = defaultdict(list)\n",
    "response = defaultdict(list)\n",
    "\n",
    "scores = defaultdict(list)\n",
    "\n",
    "for (_questions, intent, entity) in zip(questions, intents, entities):\n",
    "    question = choice(_questions)\n",
    "    \n",
    "    for row in data[intent]['df']['text']:\n",
    "        text[intent].append(row.strip() + '. ' + question.strip())\n",
    "        \n",
    "    for row in data[intent]['df'][entity]:\n",
    "        ans[intent].append((str(row)).split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "for intent in intents:\n",
    "    for row in text[intent]:\n",
    "        encoded_text[intent].append(torch.tensor([tokenizer.encode(row, add_special_tokens=True)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[   0, 6298,   20, 2367,  953, 1437, 2391,   11, 2808,   13, 5996,    4,\n",
       "         6834,  317,  116,    2]])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text['BookRestaurant'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'book The Middle East  restaurant in IN for noon. Which place?'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text['BookRestaurant'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The', 'Middle', 'East', 'in', 'IN']"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ans['BookRestaurant'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 9.9406, -2.9603,  8.3970,  ...,  1.5137,  3.6402,  6.3281],\n",
      "         [ 4.4129, -2.6280, 10.0783,  ...,  0.1638,  3.3117,  5.1125],\n",
      "         [ 5.7099, -2.0169,  3.5284,  ...,  1.9832,  2.4438,  3.9433],\n",
      "         ...,\n",
      "         [ 7.8212, -2.1413,  4.1847,  ...,  2.2004,  3.1155,  5.6780],\n",
      "         [ 6.4131, -2.7355,  3.2601,  ...,  2.8562,  3.9446,  7.4909],\n",
      "         [10.9007, -2.2821,  9.5213,  ...,  5.3673,  4.9585,  9.6004]]]),)\n",
      "['<s>Add', 'another', 'song', 'to', 'the', 'Cita', 'RomÃ¡ntica', 'playlist.', 'What', 'should', 'I', 'add', 'to?']\n",
      "(tensor([[[15.7489, -2.8303,  8.5910,  ...,  1.7682,  3.9350,  8.7439],\n",
      "         [-0.3903, -4.4944,  6.2178,  ..., -4.6338, -3.0864,  1.7993],\n",
      "         [ 5.2035, -3.0675,  6.2530,  ...,  0.7793,  3.5323,  3.9874],\n",
      "         ...,\n",
      "         [-3.0049, -3.4744,  3.5694,  ..., -1.9881,  1.2798, -2.0960],\n",
      "         [ 5.4573, -2.9659,  3.2425,  ...,  1.2363,  2.7138,  5.9312],\n",
      "         [ 9.6744, -2.5804,  9.8234,  ...,  4.0117,  4.8970,  9.3715]]]),)\n",
      "['<s>book', 'The', 'Middle', 'East', 'restaurant', 'in', 'IN', 'for', 'noon', 'Which', 'place?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[ 9.5343, -2.8868,  6.4878,  ...,  1.7897,  3.1463,  6.7134],\n",
      "         [ 4.7981, -3.0404, 12.9067,  ...,  1.1233,  3.1445,  7.0639],\n",
      "         [ 2.2193, -3.1450,  3.9553,  ...,  2.3891,  2.1724,  5.7924],\n",
      "         ...,\n",
      "         [-0.4668, -3.3859,  3.1795,  ...,  0.3377,  0.5385,  3.1542],\n",
      "         [ 2.5330, -3.0407,  5.0515,  ...,  2.9549,  1.1939,  5.4320],\n",
      "         [ 3.7759, -3.5574,  9.0519,  ..., -2.4075, -2.4362,  6.5678]]]),)\n",
      "['<s>What', 'will', 'the', 'weather', 'be', 'this', 'year', 'in', 'Horseshoe', 'Lake', 'State', 'Fish', 'and', 'Wildlife', 'Area?', 'I', 'will', 'tell', 'you', 'the', 'weather', 'for']\n",
      "(tensor([[[21.7438, -2.9834,  8.9592,  ...,  2.2917,  4.5047, 10.9875],\n",
      "         [ 6.9856, -3.0568, 11.7913,  ...,  1.4867,  4.0744,  7.7476],\n",
      "         [ 0.6561, -3.3759,  3.9537,  ..., -0.9491,  1.2122,  3.5243],\n",
      "         ...,\n",
      "         [-1.5928, -2.7800,  3.1824,  ...,  1.6826,  1.0537,  2.7808],\n",
      "         [ 0.3491, -3.9352,  4.5772,  ...,  0.9896,  1.3818,  2.2575],\n",
      "         [10.2141, -3.0805,  9.3135,  ...,  1.1027,  3.0241,  7.3346]]]),)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<s>I', 'need', 'to', 'hear', 'the', 'song', 'Aspro', 'Mavro', 'from', 'Bill', 'Szymczyk', 'on', 'Youtube', 'I', 'will', 'play</s>']\n",
      "(tensor([[[ 9.2953, -3.1320,  7.4972,  ...,  1.9521,  2.8192,  6.8728],\n",
      "         [ 3.0743, -4.4380,  6.8570,  ..., -2.9806, -3.6016,  0.8754],\n",
      "         [ 3.8165, -2.6787,  8.5445,  ...,  1.3608,  2.0882,  3.8909],\n",
      "         ...,\n",
      "         [-3.4266, -4.2963,  5.7800,  ...,  0.9868,  1.1777,  1.9817],\n",
      "         [ 4.0788, -3.2559,  6.2211,  ...,  1.1942, -1.5063,  6.8653],\n",
      "         [ 8.7478, -3.5071, 13.5857,  ...,  3.2865,  0.7605,  7.4194]]]),)\n",
      "['<s>rate', 'The', 'Lotus', 'and', 'the', 'Storm', 'zero', 'of', '6', 'I', 'will', 'rate', 'it']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[12.2224, -2.8887,  7.5436,  ...,  1.6901,  3.7063,  7.6917],\n",
      "         [ 3.5270, -3.9497,  4.6634,  ..., -2.2576, -1.8791,  4.0627],\n",
      "         [ 0.9311, -2.9260,  2.4014,  ...,  1.0564,  2.7370,  7.1741],\n",
      "         ...,\n",
      "         [ 2.8491, -3.0966,  5.5013,  ...,  4.2222,  2.6626,  5.2596],\n",
      "         [ 8.1605, -2.8175,  4.9231,  ...,  3.2799,  4.1806,  7.8245],\n",
      "         [10.1431, -2.4684, 10.9726,  ...,  5.1504,  3.8828, 10.6926]]]),)\n",
      "['<s>find', 'the', 'soundtrack', 'titled', 'This', 'Side', 'of', 'Paradise', 'What', 'should', 'I', 'look', 'for?']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[[11.8776, -3.5336,  7.4991,  ...,  0.0739,  1.7538,  7.2090],\n",
      "         [ 4.6219, -3.1463, 13.0176,  ...,  0.8489,  3.4118,  6.5938],\n",
      "         [ 1.8810, -3.6164,  7.3875,  ...,  1.6342, -0.7474,  2.8845],\n",
      "         ...,\n",
      "         [-4.1597, -4.0840,  2.6684,  ...,  1.8363, -0.2410,  2.0421],\n",
      "         [ 2.4781, -3.7044,  6.4244,  ...,  3.7775,  0.6670,  6.9013],\n",
      "         [ 6.1845, -3.9607,  9.8287,  ..., -2.5426, -3.8224,  4.9154]]]),)\n",
      "['<s>What', 'are', 'the', 'movie', 'schedule', 'at', 'Malco', 'Theatres', 'I', 'will', 'look', 'for']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    for intent in intents:\n",
    "        for i, row in tqdm(enumerate(encoded_text[intent])):\n",
    "            out = model(row)\n",
    "            actual_out = model((torch.argmax(out[0][0], dim=1)).view(1, -1))\n",
    "            print(actual_out)\n",
    "            response[intent].append(tokenizer.decode(torch.argmax(out[0][0], dim=1).tolist()).split())\n",
    "            print(response[intent][-1])\n",
    "            break\n",
    "            score = nltk.translate.bleu_score.sentence_bleu(\n",
    "                    [ans[intent][i]],\n",
    "                    response[intent][i],\n",
    "                    smoothing_function=nltk.translate.bleu_score.SmoothingFunction().method4,\n",
    "                    auto_reweigh=True\n",
    "                )\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>book The Middle East restaurant in IN for noon Where do they want to eat?'"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\" \".join(response['BookRestaurant'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "ZeroDivisionError",
     "evalue": "division by zero",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-59-74f1a5b87941>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mintent\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mintents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"For {intent}, avg. BLEU score is {sum(scores[intent]) / len(scores[intent])}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mZeroDivisionError\u001b[0m: division by zero"
     ]
    }
   ],
   "source": [
    "for intent in intents:\n",
    "    print(f\"For {intent}, avg. BLEU score is {sum(scores[intent]) / len(scores[intent])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████| 30/30 [00:03<00:00,  7.68it/s]\n"
     ]
    }
   ],
   "source": [
    "os.chdir('transformers')\n",
    "from examples.run_generation import sample_sequence\n",
    "os.chdir('..')\n",
    "_out = sample_sequence(model, 30, encoded_text['AddToPlaylist'][1][0], top_p = 0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([   0, 6298,   20, 2367,  953, 1437, 2391,   11, 2808,   13, 5996,    4,\n",
       "        6834,  317,  116,    2])"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_text['BookRestaurant'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<transformers.tokenization_roberta.RobertaTokenizer at 0x19c5dac4e48>"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>add clem burke in my playlist Pre-Party R&B Jams. What should I add to?</s>\\nmymyJPPRRRRRRRRRRRRRRRRRRRRRRRR'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(_out.squeeze(0).tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the'"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode([5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 4917,\n",
       " 12479,\n",
       " 119,\n",
       " 12601,\n",
       " 1071,\n",
       " 11,\n",
       " 127,\n",
       " 30475,\n",
       " 5048,\n",
       " 12,\n",
       " 38210,\n",
       " 248,\n",
       " 947,\n",
       " 387,\n",
       " 344,\n",
       " 7042,\n",
       " 4,\n",
       " 653,\n",
       " 197,\n",
       " 38,\n",
       " 1606,\n",
       " 7,\n",
       " 116,\n",
       " 2,\n",
       " 50118,\n",
       " 4783,\n",
       " 4783,\n",
       " 863,\n",
       " 510,\n",
       " 510,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500,\n",
       " 500]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "_out.squeeze(0).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list, {'AddToPlaylist': []})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(list,\n",
       "            {'AddToPlaylist': [['Add',\n",
       "               'another',\n",
       "               'song',\n",
       "               'to',\n",
       "               'the',\n",
       "               'Cita',\n",
       "               'RomÃ¡ntica',\n",
       "               'playlist.',\n",
       "               'Where',\n",
       "               'should',\n",
       "               'I',\n",
       "               'add?']],\n",
       "             'BookRestaurant': [['<s>book',\n",
       "               'The',\n",
       "               'Middle',\n",
       "               'East',\n",
       "               'restaurant',\n",
       "               'in',\n",
       "               'IN',\n",
       "               'for',\n",
       "               'noon',\n",
       "               'Where',\n",
       "               'do',\n",
       "               'they',\n",
       "               'want',\n",
       "               'to',\n",
       "               'eat?']],\n",
       "             'GetWeather': [['<s>What',\n",
       "               'will',\n",
       "               'the',\n",
       "               'weather',\n",
       "               'be',\n",
       "               'this',\n",
       "               'year',\n",
       "               'in',\n",
       "               'Horseshoe',\n",
       "               'Lake',\n",
       "               'State',\n",
       "               'Fish',\n",
       "               'and',\n",
       "               'Wildlife',\n",
       "               'Area?',\n",
       "               'Where?']],\n",
       "             'PlayMusic': [['<s>I',\n",
       "               'need',\n",
       "               'to',\n",
       "               'hear',\n",
       "               'the',\n",
       "               'song',\n",
       "               'Aspro',\n",
       "               'Mavro',\n",
       "               'from',\n",
       "               'Bill',\n",
       "               'Szymczyk',\n",
       "               'on',\n",
       "               'Youtube',\n",
       "               'What',\n",
       "               'should',\n",
       "               'I',\n",
       "               'play?']],\n",
       "             'RateBook': [['<s>rate',\n",
       "               'The',\n",
       "               'Lotus',\n",
       "               'and',\n",
       "               'the',\n",
       "               'Storm',\n",
       "               'zero',\n",
       "               'of',\n",
       "               '6',\n",
       "               'I',\n",
       "               'will',\n",
       "               'rate',\n",
       "               'it']],\n",
       "             'SearchCreativeWork': [['<s>find',\n",
       "               'the',\n",
       "               'soundtrack',\n",
       "               'titled',\n",
       "               'This',\n",
       "               'Side',\n",
       "               'of',\n",
       "               'Paradise',\n",
       "               'I',\n",
       "               'will',\n",
       "               'try',\n",
       "               'to',\n",
       "               'find']],\n",
       "             'SearchScreeningEvent': [['<s>What',\n",
       "               'are',\n",
       "               'the',\n",
       "               'movie',\n",
       "               'schedule',\n",
       "               'at',\n",
       "               'Malco',\n",
       "               'Theatres',\n",
       "               'Find',\n",
       "               'what?']]})"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
