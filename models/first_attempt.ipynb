{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Better speed can be achieved with apex installed from https://www.github.com/nvidia/apex.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from pytorch_pretrained_bert import GPT2Tokenizer, GPT2Model, GPT2LMHeadModel\n",
    "\n",
    "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
    "import logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.tokenization_gpt2:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-vocab.json from cache at C:\\Users\\dhruv\\.pytorch_pretrained_bert\\f2808208f9bec2320371a9f5f891c184ae0b674ef866b79c58177067d15732dd.1512018be4ba4e8726e41b9145129dc30651ea4fec86aa61f4b9f40bf94eac71\n",
      "INFO:pytorch_pretrained_bert.tokenization_gpt2:loading merges file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-merges.txt from cache at C:\\Users\\dhruv\\.pytorch_pretrained_bert\\d629f792e430b3c76a1291bb2766b0a047e36fae0588f9dbc1ae51decdff691b.70bec105b4158ed9a1747fea67a43f5dee97855c64d62b6ec3742f4cfdb5feda\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model tokenizer (vocabulary)\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# Encode some inputs\n",
    "text_1 = \"Once upon a time, in a computer lab not particularly far away, there was a guy. An awesome, cool, badass guy, yes; but just a guy nonetheless. Surprisingly, this story thingy is not about the guy, and the fact that he oozes awesome; Although that is far more interesting. It is in fact, about a girl who is yet to enter the lab. Apparently, the guy had something she wanted. Few sheets of papers, that were hers' to begin with. And so she walks up to him and calls him. He was writing something and minding his own business. He’s plenty annoyed at hearing his name. It was only moments ago that another girl had interrupted him.  Now, he happens to be a pretty weird guy, always thinking of something else entirely when while doing mundane tasks like writing or driving or walking. And so as usual he was apne thoughts mein dooba hua. Until he saw her. You see, seeing her had a weird effect on him. As if his mind lost its train of thought immediately. A new train had arrived, and it went ‘oh. She pretty’.\"\n",
    "text_2 = \"Now, he had seen her before. Multiple times, she looked plenty normal to him. Even earlier that morning, when he asked for the math tutorials she wanted back. Surely she didn't look like this subhe ko. He would have noticed. Her eyes were dark and brown and she had these cute little eyebrows knitted together in a frown; a perfectly placed birthmark above her mouth. It was this stupid birthmark that demanded the most attention. She seemed distracted, stuttering a bit and her eyes kept darting away. For some reason, she leaned towards him as she listened, with her lips slightly parted, making her even more distracting. Probably because he wasn't audible enough. She just stood there, apparently oblivious to all the distraction she was causing. Probably because the guy looked all calm and composed, regardless of what was going on in his head. Hopefully. I mean, of course he was. He's the type that walks away calmly from explosions. Like the lead of a spy movie. And when he finally thought he succeeded in ignoring her face. He noticed this cute not particularly small birthmark, the same color as the stripes on her tee. He could see it, for she was standing and he was sitting.\"\n",
    "indexed_tokens_1 = tokenizer.encode(text_1)\n",
    "indexed_tokens_2 = tokenizer.encode(text_2)\n",
    "\n",
    "# Convert inputs to PyTorch tensors\n",
    "tokens_tensor_1 = torch.tensor([indexed_tokens_1])\n",
    "tokens_tensor_2 = torch.tensor([indexed_tokens_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_pretrained_bert.modeling_gpt2:loading weights file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-pytorch_model.bin from cache at C:\\Users\\dhruv\\.pytorch_pretrained_bert\\4295d67f022061768f4adc386234dbdb781c814c39662dd1662221c309962c55.778cf36f5c4e5d94c8cd9cefcf2a580c8643570eb327f0d4a1f007fab2acbdf1\n",
      "INFO:pytorch_pretrained_bert.modeling_gpt2:loading configuration file https://s3.amazonaws.com/models.huggingface.co/bert/gpt2-config.json from cache at C:\\Users\\dhruv\\.pytorch_pretrained_bert\\4be02c5697d91738003fb1685c9872f284166aa32e061576bbe6aaeb95649fcf.085d5f6a8e7812ea05ff0e6ed0645ab2e75d80387ad55c1ad9806ee70d272f80\n",
      "INFO:pytorch_pretrained_bert.modeling_gpt2:Model config {\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"layer_norm_epsilon\": 1e-05,\n",
      "  \"n_ctx\": 1024,\n",
      "  \"n_embd\": 768,\n",
      "  \"n_head\": 12,\n",
      "  \"n_layer\": 12,\n",
      "  \"n_positions\": 1024,\n",
      "  \"vocab_size\": 50257\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load pre-trained model (weights)\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "model.eval()\n",
    "\n",
    "# If you have a GPU, put everything on cuda\n",
    "tokens_tensor_1 = tokens_tensor_1.to(device)\n",
    "tokens_tensor_2 = tokens_tensor_2.to(device)\n",
    "model.to(device)\n",
    "\n",
    "# Predict all tokens\n",
    "with torch.no_grad():\n",
    "    predictions_1, past = model(tokens_tensor_1)\n",
    "    # past can be used to reuse precomputed hidden state in a subsequent predictions\n",
    "    # (see beam-search examples in the run_gpt2.py example).\n",
    "    predictions_2, past = model(tokens_tensor_2, past=past)\n",
    "\n",
    "# get the predicted last token\n",
    "# predicted_index = torch.argmax(predictions_2[0, -1, :]).item()\n",
    "# predicted_token = tokenizer.decode([predicted_index])\n",
    "\n",
    "pred = torch.argmax(predictions_1, dim=2)[0].cpu().numpy().tolist()\n",
    "pred_text = tokenizer.decode(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 238, 50257])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' the a time, the the time game, far well from, a was a man who He old guy brilliant guy cool guy. who, but he as guy..\\n, he guy is was was not a him guy. but it guy that it\\'ses a. it,\\'s a from important than\\n\\'s about fact about a the guy who is a to be the world, She, she girl is been to wanted to She things of paper, but is all,, keep with. She she, was into to him, starts out a \"\\'s a a, sheing his own business.\\n was�s writing of, her her name called He� a a before that he girl had called him. He\\xa0, she� to be a girl good guy. and a about himself... he he his things. this. reading. whatever. He he, he, is justathetico ofandering.e.an.  he was her. He know, he her, been lot effect on him. He he he mind was its ability of thought.. He girl thought of arrived. and he was off� �,  was much�s He'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-128.1211, -131.0129, -134.6268,  ..., -140.1218, -134.4016,\n",
       "          -133.3209],\n",
       "         [-163.7341, -165.2466, -166.9875,  ..., -170.0072, -166.5462,\n",
       "          -164.6214],\n",
       "         [-144.6157, -144.9538, -148.4446,  ..., -155.3010, -143.6271,\n",
       "          -147.5490],\n",
       "         ...,\n",
       "         [-150.4981, -151.3717, -155.9007,  ..., -160.9858, -151.3975,\n",
       "          -153.6461],\n",
       "         [-126.8530, -129.2441, -135.1330,  ..., -140.6560, -131.7047,\n",
       "          -131.3890],\n",
       "         [-157.5303, -155.5184, -157.6709,  ..., -168.9378, -165.0819,\n",
       "          -152.4198]]], device='cuda:0')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[7454,\n",
       " 2402,\n",
       " 257,\n",
       " 640,\n",
       " 11,\n",
       " 287,\n",
       " 257,\n",
       " 3644,\n",
       " 2248,\n",
       " 407,\n",
       " 3573,\n",
       " 1290,\n",
       " 1497,\n",
       " 11,\n",
       " 612,\n",
       " 373,\n",
       " 257,\n",
       " 3516,\n",
       " 13,\n",
       " 1052,\n",
       " 7427,\n",
       " 11,\n",
       " 3608,\n",
       " 11,\n",
       " 37289,\n",
       " 3516,\n",
       " 11,\n",
       " 3763,\n",
       " 26,\n",
       " 475,\n",
       " 655,\n",
       " 257,\n",
       " 3516,\n",
       " 19018,\n",
       " 13,\n",
       " 47183,\n",
       " 11,\n",
       " 428,\n",
       " 1621,\n",
       " 1517,\n",
       " 88,\n",
       " 318,\n",
       " 407,\n",
       " 546,\n",
       " 262,\n",
       " 3516,\n",
       " 11,\n",
       " 290,\n",
       " 262,\n",
       " 1109,\n",
       " 326,\n",
       " 339,\n",
       " 49564,\n",
       " 274,\n",
       " 7427,\n",
       " 26,\n",
       " 4900,\n",
       " 326,\n",
       " 318,\n",
       " 1290,\n",
       " 517,\n",
       " 3499,\n",
       " 13,\n",
       " 632,\n",
       " 318,\n",
       " 287,\n",
       " 1109,\n",
       " 11,\n",
       " 546,\n",
       " 257,\n",
       " 2576,\n",
       " 508,\n",
       " 318,\n",
       " 1865,\n",
       " 284,\n",
       " 3802,\n",
       " 262,\n",
       " 2248,\n",
       " 13,\n",
       " 18626,\n",
       " 11,\n",
       " 262,\n",
       " 3516,\n",
       " 550,\n",
       " 1223,\n",
       " 673,\n",
       " 2227,\n",
       " 13,\n",
       " 20463,\n",
       " 15747,\n",
       " 286,\n",
       " 9473,\n",
       " 11,\n",
       " 326,\n",
       " 547,\n",
       " 25144,\n",
       " 6,\n",
       " 284,\n",
       " 2221,\n",
       " 351,\n",
       " 13,\n",
       " 843,\n",
       " 523,\n",
       " 673,\n",
       " 11114,\n",
       " 510,\n",
       " 284,\n",
       " 683,\n",
       " 290,\n",
       " 3848,\n",
       " 683,\n",
       " 13,\n",
       " 679,\n",
       " 373,\n",
       " 3597,\n",
       " 1223,\n",
       " 290,\n",
       " 2000,\n",
       " 278,\n",
       " 465,\n",
       " 898,\n",
       " 1597,\n",
       " 13,\n",
       " 679,\n",
       " 447,\n",
       " 247,\n",
       " 82,\n",
       " 6088,\n",
       " 25602,\n",
       " 379,\n",
       " 4854,\n",
       " 465,\n",
       " 1438,\n",
       " 13,\n",
       " 632,\n",
       " 373,\n",
       " 691,\n",
       " 7188,\n",
       " 2084,\n",
       " 326,\n",
       " 1194,\n",
       " 2576,\n",
       " 550,\n",
       " 19072,\n",
       " 683,\n",
       " 13,\n",
       " 220,\n",
       " 2735,\n",
       " 11,\n",
       " 339,\n",
       " 4325,\n",
       " 284,\n",
       " 307,\n",
       " 257,\n",
       " 2495,\n",
       " 7650,\n",
       " 3516,\n",
       " 11,\n",
       " 1464,\n",
       " 3612,\n",
       " 286,\n",
       " 1223,\n",
       " 2073,\n",
       " 5000,\n",
       " 618,\n",
       " 981,\n",
       " 1804,\n",
       " 30741,\n",
       " 8861,\n",
       " 588,\n",
       " 3597,\n",
       " 393,\n",
       " 5059,\n",
       " 393,\n",
       " 6155,\n",
       " 13,\n",
       " 843,\n",
       " 523,\n",
       " 355,\n",
       " 6678,\n",
       " 339,\n",
       " 373,\n",
       " 2471,\n",
       " 710,\n",
       " 6066,\n",
       " 502,\n",
       " 259,\n",
       " 466,\n",
       " 19981,\n",
       " 289,\n",
       " 6413,\n",
       " 13,\n",
       " 14303,\n",
       " 339,\n",
       " 2497,\n",
       " 607,\n",
       " 13,\n",
       " 921,\n",
       " 766,\n",
       " 11,\n",
       " 4379,\n",
       " 607,\n",
       " 550,\n",
       " 257,\n",
       " 7650,\n",
       " 1245,\n",
       " 319,\n",
       " 683,\n",
       " 13,\n",
       " 1081,\n",
       " 611,\n",
       " 465,\n",
       " 2000,\n",
       " 2626,\n",
       " 663,\n",
       " 4512,\n",
       " 286,\n",
       " 1807,\n",
       " 3393,\n",
       " 13,\n",
       " 317,\n",
       " 649,\n",
       " 4512,\n",
       " 550,\n",
       " 5284,\n",
       " 11,\n",
       " 290,\n",
       " 340,\n",
       " 1816,\n",
       " 564,\n",
       " 246,\n",
       " 1219,\n",
       " 13,\n",
       " 1375,\n",
       " 2495,\n",
       " 447,\n",
       " 247,\n",
       " 13]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indexed_tokens_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
